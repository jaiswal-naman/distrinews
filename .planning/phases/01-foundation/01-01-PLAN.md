# Phase 1: Foundation — Detailed Plan

**Phase Goal:** Build the core model and dataset components that all training scripts will use.

**Requirements Covered:** MOD-01, MOD-02, MOD-03, MOD-04, DAT-01, DAT-02, DAT-03, DAT-04, DEP-02

---

## Task Breakdown

### Task 1: Create Project Structure
**File:** Directory structure
**What:** Create the folder structure per spec
**Why:** Clean organization makes the project maintainable

```
distrinews/
├── training/
├── inference/
├── checkpoints/
├── benchmarks/
```

### Task 2: Create requirements.txt
**File:** `requirements.txt`
**What:** List all Python dependencies with versions
**Why:** Reproducible environment is essential for ML projects

Dependencies needed:
- torch >= 2.1.0
- transformers >= 4.36.0
- datasets >= 2.16.0
- fastapi >= 0.109.0
- uvicorn >= 0.27.0
- tqdm >= 4.66.0

### Task 3: Create Model Wrapper
**File:** `training/model.py`
**What:** DistilBERT classifier that wraps HuggingFace model
**Why:** Encapsulates model creation, makes it easy to swap models later

Components:
- `NewsClassifier` class
- `__init__`: Load DistilBERT with 4 output classes
- Forward pass returns logits
- Method to get model for DDP wrapping

Key concepts to explain:
- What is DistilBERT
- Why 4 classes (AG News categories)
- What are logits
- Why we wrap instead of using directly

### Task 4: Create Dataset Loader
**File:** `training/dataset.py`
**What:** Load AG News, tokenize, create PyTorch Dataset
**Why:** Data pipeline is foundation of any ML project

Components:
- `load_agnews()` function to get raw data
- `AGNewsDataset` class (PyTorch Dataset)
- `__init__`: Store tokenizer and data
- `__len__`: Return dataset size
- `__getitem__`: Return single tokenized sample
- `get_tokenizer()` function

Key concepts to explain:
- What is tokenization
- Why we need padding/truncation
- What is attention_mask
- How PyTorch Dataset works
- Why DistributedSampler needs this interface

---

## Success Criteria

1. ✓ `python -c "from training.model import NewsClassifier"` works
2. ✓ `python -c "from training.dataset import AGNewsDataset, load_agnews"` works
3. ✓ Model outputs shape (batch_size, 4)
4. ✓ Dataset returns dict with input_ids, attention_mask, labels
5. ✓ All code has explanatory comments

---

## Files to Create

| File | Purpose | Lines (est) |
|------|---------|-------------|
| `requirements.txt` | Dependencies | 15 |
| `training/__init__.py` | Package marker | 1 |
| `training/model.py` | Model wrapper | 80 |
| `training/dataset.py` | Data loading | 120 |

---

## Execution Order

1. Create directory structure
2. Create requirements.txt
3. Create training/__init__.py
4. Create training/model.py (with tests)
5. Create training/dataset.py (with tests)
6. Verify everything works together

---
*Plan created: 2025-01-30*
