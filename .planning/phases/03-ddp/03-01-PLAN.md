# Phase 3: Distributed Training (DDP) — Detailed Plan

**Phase Goal:** Convert single-GPU training to multi-GPU DDP training.

**Requirements Covered:** DDP-01 through DDP-09, CHK-01 through CHK-04, LCH-01 through LCH-03

---

## This Is The Core Phase

Everything before this was preparation:
- Phase 1: Built model and data loading
- Phase 2: Built training loop

This phase transforms that into DISTRIBUTED training.

---

## Task Breakdown

### Task 1: Create DDP Training Script
**File:** `training/train_ddp.py`
**What:** Full DDP training with all concepts explained
**Why:** The main learning objective of this project

Key DDP components to explain:
1. init_process_group() — Initialize distributed environment
2. RANK, LOCAL_RANK, WORLD_SIZE — Process identification
3. DistributedDataParallel wrapper — Gradient synchronization
4. DistributedSampler — Data sharding
5. set_epoch() — Proper shuffling
6. Rank 0 logging — Avoid duplicate output
7. Rank 0 checkpointing — Avoid file conflicts
8. destroy_process_group() — Clean shutdown

### Task 2: Create Launch Script
**File:** `training/run_ddp.sh`
**What:** Shell script to launch DDP training with torchrun
**Why:** torchrun handles process spawning and environment setup

---

## Success Criteria

1. ✓ Process group initializes with NCCL backend
2. ✓ Each GPU uses correct LOCAL_RANK
3. ✓ Model wrapped with DDP
4. ✓ DistributedSampler shards data correctly
5. ✓ set_epoch() called every epoch
6. ✓ Logs appear only from rank 0
7. ✓ Checkpoint saved only by rank 0
8. ✓ Clean shutdown with destroy_process_group
9. ✓ Code has comments explaining EVERY DDP concept

---

## Files to Create

| File | Purpose | Lines (est) |
|------|---------|-------------|
| `training/train_ddp.py` | DDP training script | 450 |
| `training/run_ddp.sh` | torchrun launcher | 30 |

---
*Plan created: 2025-01-30*
